---
title: "Session 3 Uncertainty visualizations"
author: "Xiaoying Pu, Lace Padilla"
date: "3/3/2022"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(modelr)
library(brms)
library(tidybayes)
library(ggdist)
library(distributional)
library(ordinal)
library(ggrepel)
library(glue)
library(dabestr)

theme_set(theme_tidybayes())
```

## Introduction

We will revisit Datasets 1 and 2 from last time, focusing on exploring
the **visualizations** instead of the analysis. By the end of this
tutorial, you will learn to:

1.  Apply the **Grammar of Graphics** to build simple plots
2.  Identify the types of **uncertainty** from model outputs and modify
    code to visualize outputs and uncertainty
3.  Use resources to find options for making a plot more **expressive**
    and evaluate its **usability**

### Pre-survey

What makes a plot? How do you systematically describe a plot, like
linguists using **grammar** to describe a language?

Go to ==TODO== (survey link)

It's OK if you're sure about your answers! It's all part of the plan ðŸ¤”

\[Review survey response and adjust timing accordingly\]

------------------------------------------------------------------------

## Activity 1: exploring data

### Load the data

```{r}
(df1 <- read_csv("data/blinded.csv") %>%
  filter(experiment == 1) %>%
  mutate(
    condition = factor(condition), 
    participant_id = paste0("P", str_pad(row_number(), 2, pad = "0")))
)
```

### Explore the data with ggplot

We modeled how `condition` (graph or no graph) affected `effectiveness`,
a Likert scale rating from 1 to 9.

To start out, create a scatterplot of effectiveness vs. condition. Your
plot will end up like this

![](images/000005.png)

The ggplot specification is the following:

```{r}
df1 %>%
  ggplot(aes(
    x = effectiveness, 
    y = condition)) +  
  geom_point() +
  scale_x_continuous(breaks = 1:9)
```

Let's break it down in terms of the (layered) Grammar of Graphics
concepts so that we can understand how ggplot works

-   `ggplot`: creates the "default layer"

-   `geom_point()`: Mark/geometry of type point, also a layer

-   `aes`: Aesthetics, aka visual channel encoding. If `aes()` is within
    `ggplot`, the mappings apply to all layers, unless other layers
    override them.

        x <- effectiveness 
        y <- condition

REFLECTION: is this plot good? Is it showing all the data points?

Idea: place the points with some random jitter

```{r}
jitterer <- position_jitter(height = 0.1, seed = 1)
```

Let's write the Grammar of Graphics spec again

    ggplot
        x <- effectiveness 
        y <- condition
        color <- condition
    geom_point

```{r}
df1 %>%
  ggplot(aes(
    x = effectiveness, 
    y = condition,
    color = condition)) +     # EXERCISE: add color
  geom_point(position = jitterer) +       # EXERCISE: change to jitter
  scale_x_continuous(breaks = 1:9)
```

Common problem and how to fix:

```{r}
df1 %>%
  ggplot() +  
  geom_point(
    x = effectiveness, 
    y = condition,
    color = condition, 
    position = jitterer)
```


## Activity 2: t-test/simple linear regression

|                         | Frequentist | Bayesian |
|-------------------------|-------------|----------|
| Table summary           |             |          |
| Point estimate          |             |          |
| Interval-parameter      |             |          |
| Interval-prediction     |             |          |
| Distribution-parameter  |             |          |
| Distribution-prediction |             |          |

### Frequentist and Bayesian models

t-test is equivalent to a linear model estimating two means. Using `lm`
because the output is easier to wrangle (good explainer:
<https://lindeloev.github.io/tests-as-linear/#61_one-way_anova_and_kruskal-wallis>)

```{r}
(m_t_test <- lm(effectiveness ~ - 1 + condition, data = df1))
```

The Bayesian t-test:

```{r}
(m_t_test_bayesian <- readRDS("../02-bayesian_stats/rds/dataset1.brm.bayesiant.rds"))
```

### Build CI and PI dataframes; table views

Baseline: table/textual reporting

For **confidence interval (CI) \~ parameter uncertainty**

-   `estimate` column is for the estimates of the group means
-   `std.error` is the standard error. Math:
    $\sqrt{MSE \left(\frac{1}{n} + \frac{(x_h - \bar{x})^2}{\sum{(x_i - \bar{x})^2}}\right)}$
-   95% CI ($\alpha = 0.05$) is
    $t_{1 - 0.05/2, df} \times \sqrt{MSE \left(\frac{1}{n} + \frac{(x_h - \bar{x})^2}{\sum{(x_i - \bar{x})^2}}\right)}$,
    where the t-multiplier $t_{1 - 0.05/2, df}$ is calculated as
    `qt(1 - 0.05/2, df)`.

We won't calculate CI intervals directly. Instead we'll calculate the
standard errors and use `distributional::dist_student_t` and `ggdist`
functions later.

```{r}
(means_t_freq <- m_t_test %>% 
   tidy() %>%
   mutate(condition = str_remove(term, "condition"), .before = 1) %>%
   select(-term)
)
```

For the Bayesian model, we get draws from the posterior predictive
*expectations* (means) of all the parameters of the t-distribution
(mu/.epred, sigma, nu). There is credible interval (CI) for Bayesian
models, but we're not summarizing intervals yet.

```{r }
(means_t_bayes <- data_grid(df1, condition) %>%
  add_epred_draws(m_t_test_bayesian, newdata = . ,dpar = TRUE))
```

From a bunch of samples to a summary table:

```{r }
means_t_bayes %>%
  median_qi(.epred, .width = c(0.95))
```

**Prediction interval (PI) \~ prediction uncertainty**

For the frequentist model. `.fitted` is the same as the `estimate`
above. `.se.pred` times the t-multiplier is the PI.

NOTE on calculating the `se.pred`:
<https://online.stat.psu.edu/stat501/lesson/3/3.3> gives a good
explanation, and to translate the mathematical notation to R code:

-   MSE is `sum(residuals(m_t_test))/df.residual(m_t_test)`
-   Variance due to estimating the mean `.se.fit^2`
-   Variance due to variation in the response variable
    `residual.scale^2`
-   The standard error of prediction is the square root of the two
    variances combined

```{r}
# predict() returns $residual.scale
preds_t_freq_obj <- predict(m_t_test, newdata = data_grid(df1, condition), 
                     se.fit=TRUE, interval = "prediction", level = 0.95)

(preds_t_freq <- m_t_test %>%
  augment(newdata = data_grid(df1, condition), se_fit = TRUE) %>%
  mutate(
    .se.pred = sqrt(.se.fit ^ 2 + preds_t_freq_obj$residual.scale^2), # NOTE
    df = preds_t_freq_obj$df
  ) 
)
```

Predictions for the Bayesian model: get draws from the posterior
predictive distribution (different from `epred` from above!)

```{r}
(preds_t_bayes <- data_grid(df1, condition) %>%
    add_predicted_draws(m_t_test_bayesian))

preds_t_bayes %>%
  median_qi(.width = 0.95)
```

### Point estimates of the mean

**Frequentist** `lm()` model, plot estimates of the mean effectiveness
by condition:

First, use "pseudo" Grammar of Graphics/ggplot to articulate what you
want

    geom_point
      x <- estimate 
      y <- condition

```{r }
means_t_freq %>%
  ggplot(aes(x = estimate, y = condition)) + 
  geom_point() +
  scale_x_continuous(breaks = 1:9, limits = c(1, 9)) + 
  labs(title = "Frequentist lm(), estimated effectiveness by condition") +
  NULL
```

**Bayesian** and frequentist combined, because why not?

    geom_point
      x <- estimate 
      y <- condition
      color <- model

```{r}
# EXERCISE: add the frequentist point estimates to the plot
(p_pt_est <- means_t_bayes %>%
  median_qi(.epred, .width = c(0.95)) %>%
  ggplot(aes(x = .epred, y = condition)) + 
  geom_point(aes(color = "Bayesian")) + 
  geom_point(
    aes(x = estimate, 
    color = "Frequentist"), 
    data = means_t_freq, 
    position = position_nudge(y = -0.1)) + 
  scale_x_continuous("Effectiveness", breaks = 1:9, limits = c(1, 9)) + 
  scale_color_manual("Model", 
                     breaks = c("Bayesian", "Frequentist"), 
                     values  = c("#45829f", "#000000") ) +
  labs(title = "Estimated effectiveness by condition") +
  NULL
)

# REFLECTION: what just happened? How is that legend generated?
# TOOL! https://yjunechoe.github.io/posts/2022-03-10-ggplot2-delayed-aes-1/
# layer_data(p_pt_est, i = 1)
```

REFLECTION: point estimates don't show uncertainty at all. (So this
model comparison isn't that great)

### Intervals - parameter and prediction uncertainty

Add interval for uncertainty. In the **frequentist** world, the
*confidence distribution* for the parameters follows the student-t
distribution.
<https://mjskay.github.io/ggdist/articles/freq-uncertainty-vis.html>

We can use `distributional::dist_student_t` to specify a student-t
distribution, all we need are the three parameters we calculated earlier

-   df: degree of freedom (N-2)
-   mu: estimate/.fitted for the mean
-   sigma: standard error

Note: `dist_student_t` only works with `xdist` aesthetic from the
`ggdist` package.

    stat_pointinterval
      xdist <- student_t(df, mu, sigma) 
      y <- condition

REFLECTION: how would you plot it if you aren't using
`xdist`/`dist_student_t`?

REFLECTION: why is it `stat_*`, not `geom_*`?

```{r }
means_t_freq %>%
  ggplot(aes(
    xdist = dist_student_t( 
      df = df.residual(m_t_test), 
      mu = estimate, 
      sigma = std.error), 
    y = condition)) + 
  # EXERCISE: add 0.66 as another CI level
  stat_pointinterval(.width = c(0.66, 0.95)) +
  scale_x_continuous("effectiveness", breaks = 1:9, limits = c(1, 9)) + 
  labs(title = "Frequentist lm(), estimated effectiveness by condition",
       subtitle = "Confidence interval widths: 0.66 and 0.95") +
  NULL
```

The **prediction distribution** also follows the t-distribution, so we
only need to change the `sigma` parameter. Show that PI is wider than
the CI in the plot below!

```{r}
preds_t_freq %>%
  ggplot(aes(y = condition)) + 
  stat_interval(aes(
    xdist = dist_student_t(
      df = df,
      mu = .fitted, 
      sigma = .se.pred) 
  )) + 
  # EXERCISE: put CI (dot and whisker) in this plot
  stat_pointinterval(aes(
    xdist = dist_student_t( # EXERCISE: what does this mean?
      df = df,
      mu = .fitted,
      sigma = .se.fit)),
    position = position_nudge(y = -.2)) +
  # EXERCISE: put raw data in this plot
  geom_point(
    aes(x = effectiveness), 
    data = df1, position = jitterer) +
  scale_x_continuous("effectiveness", breaks = 1:11, limits = c(0, 11)) + 
  scale_color_brewer("Predictive interval") + 
  labs(title = "Frequentist lm(), estimated effectiveness by condition",
       subtitle = "Confidence interval widths: 0.66 and 0.95") +
  NULL
  
```

Replicate the CI+PI plot above, but with the **Bayesian** model

EXERCISE: Visualize parameter uncertainty, predictive uncertainty, and
the intrinsic uncertainty in data:

      (which visual element?) <- (Parameter uncertainty)
      (which visual element?) <- (Predictive uncertainty)
      (which visual element?) <- (Intrinsic uncertainty in data)

```{r}
df1 %>%
  # EXERCISE: fill this out
  ggplot(aes(x = effectiveness, y = condition)) + 
  stat_interval(aes(x = .prediction), data = preds_t_bayes) +
  stat_pointinterval(aes(x = .epred), data = means_t_bayes,
                     .width = c(.66, .95),
                     position = position_nudge(y = -0.2)) +
  geom_point(position = jitterer) +
  # END EXERCISE
  scale_x_continuous(breaks = 1:9)+
  scale_color_brewer("Predictive interval") + 
  labs(title = "Bayesian lm(), estimated effectiveness by condition",
       subtitle = "Credible interval widths: 0.66 and 0.95") +
  NULL
```

### Distributions - parameter and prediction uncertainty

EXERCISE: What encoding do you prefer? How to look for more options?

<https://mjskay.github.io/ggdist/>

REFLECTION: how would you categorize all these options? 

- continuous (`stat_slab`) vs. discrete (`stat_dots`)
- intervals (`stat_interval`) vs. distribution (`stat_slab`) vs. both (`stat_halfeye`)

```{r}
means_t_freq %>%
  ggplot(aes(y = condition)) +
  # EXERCIESE: add raw data
  geom_point(
    aes(
      x = effectiveness,
      y = condition),
    data = df1, position = jitterer, alpha = .5) +
  stat_halfeye(             # EXERCIESE: change to stat_dots
    aes(xdist =dist_student_t(
      df = df.residual(m_t_test), 
      mu = estimate, 
      sigma = std.error)),
    scale = 0.5, fill = "#8c96c6",
    position = position_nudge(y = 0.15)) + 
  
  # EXERCISE: add a label for confidence distribution
  geom_label_repel(
    aes(estimate + 0.25, y = condition), 
    data = means_t_freq %>% slice_tail(),
    label = "CI of the mean", box.padding = 1,
    position = position_nudge(y = 0.3), max.overlaps = Inf, seed = 15) +
  # EXERCISE: add a label for the raw data
  geom_label_repel(
    aes(x = estimate, y = condition),
    data = means_t_freq %>% slice_head(),
    label = "Jittered raw data", box.padding = 1,
    position = position_nudge_repel(y = -0.2), seed = 1) +
  scale_x_continuous(breaks = 0:10, limits = c(0, 10)) + 
  labs(title = "Frequentist lm(), confidence distribution of effectiveness by condition") + 
  NULL

```

Prediction distribution for the frequentist model:

```{r}
preds_t_freq %>%
  ggplot(aes(y = condition)) + 
  stat_slab(
    aes(xdist = dist_student_t(
      df = df, 
      mu = .fitted, 
      sigma = .se.pred)),
    slab_color="#9ECAE1", fill = NA, scale = 0.7) +
  geom_point(
    aes(
      x = effectiveness,
      y = condition),
    data = df1, position = jitterer, shape = 21, size =2) +
  stat_pointinterval(            
    aes(xdist =dist_student_t(
      df = df.residual(m_t_test), 
      mu = estimate, 
      sigma = std.error)),
    data = means_t_freq,
    scale = 0.5, fill = "#8c96c6",
    position = position_nudge(y = 0.15)) + 
  scale_x_continuous(breaks = 1:9) +
  labs(title = "Frequentist lm(), effectiveness by condition",
       subtitle = "Curves show the prediction distributions\nIntervals show confidence intervals") + 
  NULL
```

This type of plot is sometimes called Kruschke style in Bayesian context https://solomonkurz.netlify.app/post/2018-12-20-make-rotated-gaussians-kruschke-style/. The
bunch of density lines shows both parameter and predictive uncertainty (why?)

```{r}
means_t_bayes %>%
  sample_draws(30) %>%
  ggplot(
    aes(y = condition)) + 
  stat_slab(
    aes(xdist = dist_student_t(nu, mu, sigma)),
    slab_color="#9ECAE1", alpha = 0.1, fill = NA, scale = 0.7) +
  # stat_pointinterval(
  #   aes(x = mu), 
  #   position = position_nudge(y = 0.15)) +
  geom_point(
    aes(x = effectiveness), data = df1,
    shape = 21, size = 2,
    position = jitterer) +
  scale_x_continuous(breaks = 1:9, limit = c(0, 11)) + 
  labs(title = "Bayesian t-test, effectiveness by condition",
       subtitle = "Curves show the prediction distributions\nIntervals show credible intervals") + 
  NULL
```

REFLECTION: should you show confidence or prediction distribution, or both? 
Should you use intervals or distributions for uncertainty?


## Activity 3: Ordinal linear regression

Now you will design your own plots for the frequentist and Bayesian ordinal linear regression. You have data exploration and model output summaries below. 


### Considerations and resources

- What does the model look like? How does it work?
- What are the parameter and prediction uncertainty?
- How do I want to visualize the uncertainties? (Interval? distribution?)

- Bayesian OLR <https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/monsters-and-mixtures.html#ordered-categorical-outcomes>


### Data exploration and how cumulative logit works

The same `df1` data, but summarized to show proportion of each effectiveness level (1-9).

```{r}
df1 %>%
  count(condition, effectiveness, name = "n")  %>%
  add_row(condition = "graph", effectiveness = 2, n = 0) %>%
  complete(condition, effectiveness, fill = list(n = 0)) %>%
  ggplot(aes(x = effectiveness, y = n, color = condition)) + 
  geom_point() + 
  geom_line() + 
  scale_x_continuous(breaks = 1:9) + 
  NULL
```

Building up to the ordinal linear regression model... introducing the
cumulative proportions ("odds"), and cumulative log odds (computed with
`logit()`)

```{r}
logit <- function(x) log(x / (1 - x))

(cumu_df1 <- df1 %>%
  count(condition, effectiveness, name = "n") %>%
  add_row(condition = "graph", effectiveness = 2, n = 0) %>%
  complete(condition, effectiveness, fill = list(n = 0)) %>%
  group_by(condition) %>%
  mutate(
    pr_k = n / sum(n),
    cum_pr_k = cumsum(pr_k),
    cum_pr_k_1 = cum_pr_k - (pr_k),
    logit_cum_pr_k = logit(cum_pr_k))
)
```

The first figure, but on log cumulative odds scale. Can you interpret the labeled numbers?

```{r}
cumu_df1 %>%
  filter(!is.infinite(logit_cum_pr_k)) %>%
  ggplot(aes(x = effectiveness, y = logit_cum_pr_k, color = condition, label = round(logit_cum_pr_k, 2))) + 
  geom_vline(xintercept = 6) + 
  geom_label_repel(data = cumu_df1 %>% filter(effectiveness == 6)) + 
  geom_point() + 
  geom_line() + 
  scale_x_continuous(breaks = 1:9) + 
  ylab("log-cumulative-odds")
```

Once we have the estimates in the cumulative log odds space, can do
inverse logit (`brms::inv_logit_scaled()`), and get proportions out of
the cumulative proportions. The vertical straight lines are the individual 
proportions at each effectiveness level.

```{r}
cumu_df1 %>%
  ggplot(aes(x = effectiveness, y = cum_pr_k)) + 
  geom_line() +
  geom_segment(aes(
    x = effectiveness, 
    xend = effectiveness,
    y = cum_pr_k, 
    yend = cum_pr_k_1),
    color = "grey") +
  geom_segment(aes(
    x = effectiveness - 1, 
    xend = effectiveness,
    y = cum_pr_k_1, 
    yend = cum_pr_k_1),
    linetype = "dotted", color = "grey") +
  geom_point() +
  scale_x_continuous(breaks = 1:9) + 
  facet_grid(~condition)
```

### Frequentist model

Frequentist

```{r}
(m_polr <- df1 %>% 
  mutate(effectiveness = ordered(effectiveness, levels = as.character(1:9))) %>% 
  MASS::polr(effectiveness ~ condition, data = ., Hess=TRUE) 
)
```

```{r}
tidy(m_polr)
```

```{r}


tidy(m_polr)  %>%
  mutate(delta = first(estimate)) %>%
  dplyr::slice(-1) %>%
  crossing(condition = data_grid(df1, condition) %>% pull(condition))  %>%
  mutate(estimate = ifelse(condition == "no_graph", estimate + delta, estimate)) %>%
  select(-delta) %>%
  mutate(across(estimate:std.error, ~ inv.logit(.))) %>%
  arrange(condition) %>%
  add_row(estimate = 1, condition = "no_graph") %>%
  add_row(estimate = 1, condition = "graph") %>%
  group_by(condition) %>%
  mutate(p = estimate - lag(estimate, default = 0)) %>%
  ungroup()  %>%
  ggplot(aes(x = condition, y = p, group = term)) + 
  # stat_lineribbon(aes(
  #   x = condition,
  #   ydist = dist_normal(
  #    mu = p,
  #    sigma = std.error),
  #   fill_ramp = stat(.width)), .width = ppoints(50), fill = "#2171b5"
  #   ) +
  geom_line() + 
  facet_grid(~ term) + # EXERCISE: uncomment to see data better
 # scale_fill_ramp_continuous(range = c(1, 0)) +
  NULL
  
  
```


Showing estimates on the cumulative log odds scale

```{r}
tidy(m_polr) %>%
  dplyr::slice(-1) %>%
  crossing(condition = c("graph", "no_graph")) %>%
  mutate(estimate = 
           ifelse(condition == "no_graph", 
                  estimate + condition_no_graph$estimate, 
                  estimate)) %>%
  ggplot() + 
  stat_lineribbon(aes(
    x = condition,
    ydist = dist_normal(
     mu = estimate,
     sigma = std.error),
    fill_ramp = stat(.width)), .width = ppoints(50), fill = "#2171b5"
    ) +
  facet_grid(~ term) + # EXERCISE: uncomment to see data better
 scale_fill_ramp_continuous(range = c(1, 0)) +
  # scale_fill_viridis_d()
  NULL
```

Going back to the data space... show predicted proportions and
uncertainty

```{r}
# EXERCISE: how to show uncertainty with this?

as.data.frame(predict(m_polr, type.predict = "prob", se.fit = TRUE))  %>%
  bind_cols(df1 %>% select(effectiveness, condition))  %>%
  group_by(effectiveness, condition) %>% 
  slice_head() %>%
  ungroup()   %>%
  ggplot(aes(x = effectiveness, y = fit, fill = condition)) + 
  geom_col(position = "dodge") + 
  # geom_pointinterval(   # EXERCISE: put in the interval 
  #   aes(ymin = fit - 1.96 * se.fit, ymax = fit + 1.96 * se.fit),
  #   position = "dodge") + 
  scale_x_continuous(breaks = 1:9) + 
  ylab("Proportion")
```

### Bayesian OLR

There're starter values and stuff, not bothered just yet
<https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/monsters-and-mixtures.html#ordered-categorical-outcomes>

```{r dat1-bayesian-model3}
(m_olr_bayes <- readRDS("../02-bayesian_stats/rds/dataset1.brm.olr1.rds"))
```

Get posterior estimates for the parameters and predictions (outcome on
the scale of 1...9)

```{r}
(means_df1_olr <- expand(df1, condition) %>%
  add_epred_draws(m_olr_bayes, dpar = TRUE) %>%
   rename(effectiveness = .category)
)
```

```{r}
means_df1_olr %>%
  group_by(.draw) %>%
  filter(cur_group_id()== 1)
```

```{r}
(preds_df1_olr <- expand(df1, condition) %>%
    add_predicted_draws(m_olr_bayes) %>%
    select(-c(.chain, .iteration)) %>%
   drop_na()
 )

preds_df1_olr %>%
  ungroup() %>%
  expand(nesting(.draw))

```

EXERCISE: why does this look different from the frequentist version?

```{r}
(df1_prop <- df1 %>%
  count(condition, effectiveness) %>%
  group_by(condition) %>%
  mutate(prop = n/sum(n))
)

means_df1_olr %>%
  ggplot(aes(x = condition, y = .epred)) + 
  stat_lineribbon()  + 
  geom_point(aes(x = condition, y = prop), data = df1_prop) + 
  scale_fill_brewer() + 
  facet_grid(~ effectiveness) +
  theme(axis.text.x = element_blank()) + 
  xlab("Condition: graph - no graph") + 
  ylab("Proportion")
```

EXERCISE (challenge): follow
<https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/monsters-and-mixtures.html#ordered-categorical-outcomes>
to construct a stacked version

```{r}
(means_df1_olr_compare <- means_df1_olr %>%
  group_by(effectiveness) %>%
  compare_levels(variable = .epred, by = condition ) 
)

(df1_prop_diff <- df1_prop %>%
  pivot_wider(id_col = effectiveness, names_from = condition, values_from = prop) %>%
  drop_na() %>%
  mutate(diff = no_graph - graph)
)

means_df1_olr_compare %>%
  ggplot(aes(x = effectiveness, y = .epred)) + 
  stat_interval(.width = c(0.95)) +  # EXERCISE: can you be more expressive with uncertainty?
  geom_hline(                        # EXERCISE: add reference line y = 0
    yintercept = 0, linetype = "dashed", color = "grey") + 
  geom_point(aes(y = diff), data= df1_prop_diff) +  #EXERCISE: add raw data
  scale_color_brewer() + 
  ylab("no graph - graph") + 
  labs(title = "Difference in proportions: no graph - graph") + 
  NULL
```

```{r}
preds_df1_olr %>%
  count(condition, .prediction) %>%
  ggplot(aes(.prediction, n, fill = condition)) +
  geom_col(position = "dodge")

```

## Packages Information

```{r}
sessionInfo()
```
