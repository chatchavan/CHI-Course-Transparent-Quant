---
title: "Session 3 Uncertainty visualizations"
author: "Xiaoying Pu, Lace Padilla"
date: "3/3/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(modelr)
library(brms)
library(tidybayes)
library(ggdist)
library(distributional)
library(ordinal)
library(ggrepel)
library(glue)
library(dabestr)
library(rethinking)

theme_set(theme_tidybayes())
```


## Dataset 1 Blinded with Science

### Load the data

```{r}
(df1 <- read_csv("data/blinded.csv") %>%
  filter(experiment == 1) %>%
  mutate(
    condition = factor(condition), 
    participant_id = paste0("P", str_pad(row_number(), 2, pad = "0")))
)
```


### Explore the data with ggplot

We modeled how `condition` (graph or no graph) affected `effectiveness`, a Likert scale rating from 1 to 9.

```{r}
df1 %>%
  ggplot(aes(
    x = effectiveness, 
    y = condition)) +  # EXERCISE: add color
  geom_point() +       # EXERCISE: change to jitter
  scale_x_continuous(breaks = 1:9)
```

Try aggregation


```{r}
df1 %>%
  count(effectiveness, condition, name = "n") %>%
  pivot_wider(names_from = condition, values_from = n, values_fill = 0)
```

Visualize the aggregated dataset 

```{r}
df1 %>%
  count(condition, effectiveness, name = "n") %>%
  ggplot(aes(x = effectiveness, y = n, color = condition)) + 
  geom_point() +     # EXERCISE: change to line chart (geom_line)
  # EXERCISE: point + line??
  scale_x_continuous(breaks = 1:9)
```

## Dataset 1, frequentist land 


### Wilcoxon 

```{r}
m_wilcox <-  wilcox.test(
    effectiveness ~ condition, 
    data = df1, paired = FALSE, conf.int = TRUE)

tidy(m_wilcox)
```

Reflection: is this point estimate and interval useful?

```{r}
tidy(m_wilcox) %>%
  ggplot(aes(x = estimate, y = 0)) + 
  geom_pointinterval(aes(xmin = conf.low, xmax = conf.high)) +
  scale_x_continuous(breaks = -1:9, limits = c(-1, 9))
```


### t-test

Baseline: table/textual reporting

```{r }
m_t_test <- t.test(
  effectiveness ~ condition, 
  data = df1, paired = FALSE, conf.int = TRUE)
```


```{r }
m_t_test
```

t-test is equivalent to a linear model with a slope and intercept $y_i = \beta_0 + \beta_1 x_i$ (`1 + condition` below) so using `lm` because the output is easier to wrangle (good explainer: https://lindeloev.github.io/tests-as-linear/#61_one-way_anova_and_kruskal-wallis)

```{r}
(m_t_test <- lm(effectiveness ~ 1 + condition, data = df1))
```

Tidy view of the t-test results

```{r }
m_t_test %>%
  tidy()
```

Point estimates

```{r }
tidy(m_t_test) %>%
  ggplot(aes(x = estimate, y = term)) + 
  geom_point()
```

Add interval for uncertainty

```{r }
tidy(m_t_test) %>%
  ggplot(aes(xdist = dist_student_t( # EXERCISE: what does this mean?
    df = df.residual(m_t_test), 
    mu = estimate, sigma = std.error), y = term)) + 
  stat_pointinterval() 
  # scale_color_brewer()
```
Make predictions with `augment`

```{r }
(aug_df1 <- df1 %>%
  data_grid(condition) %>%
  augment(m_t_test, newdata = ., se_fit = TRUE)
)
```

EXERCISE: why are the densities so narrow? Does it make sense? What encoding do you prefer?

```{r }
aug_df1 %>%
  ggplot(aes(y = condition)) +
  # geom_jitter( # EXERCIESE: add raw data
  #   aes(
  #     x = effectiveness, 
  #     y = condition), 
  #   data =df1, height = 0.05, alpha = .5) +  
  stat_halfeye( # EXERCIESE: change to stat_dots
    aes(xdist =dist_student_t(
      df = df.residual(m_t_test), 
      mu = .fitted, sigma = .se.fit)),
    scale = 0.6, fill = "#8c96c6",
    position = position_nudge(y = 0.1)) + 
  geom_label_repel(aes(.fitted, y = condition), data = aug_df1 %>% slice_tail(),
                   label = "Confidence distribution of mu", box.padding = 1, 
                   position = position_nudge(y = 0.1), max.overlaps = Inf) + 
  # geom_label_repel( # EXERCISE: add labels for the raw data
  #   aes(.fitted, y = condition), 
  #   data = aug_df1 %>% slice_head(),
  #   label = "Jittered raw data", box.padding = 1,
  #   position = position_nudge_repel(y = -0.2)) +
  NULL
```
Show fit line with data

```{r}
(coefficients <- m_t_test %>%
  tidy() %>%
  mutate(term = c("intercept", "slope")) %>%
  select(term, estimate) %>% 
  deframe()
)
```


```{r}
df1 %>% 
  ggplot(aes(x = ifelse(condition == "graph", 0, 1), y = effectiveness)) + 
  stat_lineribbon(aes(
    x = ifelse(condition == "graph", 0, 1), y = NULL, 
    ydist = dist_student_t(
      df = df.residual(m_t_test), 
      mu = .fitted, sigma = .se.fit)), 
    data = aug_df1, alpha = 0.3, fill = "#8c96c6") +
  geom_count(color = "gray45") + 
  geom_label_repel(
    data = tibble(x =0.7, y = 0.7 * 0.18 + 6.57),
    mapping = aes(x = x, y = y),
    label = glue("y = {format(round(coefficients[\"intercept\"], 2), nsmall=2)} + {format(round(coefficients[\"slope\"], 2), nsmall=2)}x"), color = "#8c96c6", 
    box.padding = 2) + 
  labs(
    title = "The t-test/lm fit line",
    subtitle = "Is it good for discrete data?\nAlso, don't encode count as dot size") + 
  xlab("condition") +
  scale_x_continuous(breaks = c(0,1), labels = c("graph (x = 0)", "no graph (x = 1)"), limits = c(-0.5, 1.5)) 
```

### Ordinal linear regression

https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/monsters-and-mixtures.html#ordered-categorical-outcomes

We've seen this before

```{r}
df1 %>%
  count(condition, effectiveness, name = "n") %>%
  ggplot(aes(x = effectiveness, y = n, color = condition)) + 
  geom_point() + 
  geom_line() + 
  NULL
```

Building up to the ordinal linear regression model... introducing the cumulative proportions ("odds"), and cumulative log odds 

```{r}
(cumu_df1 <- df1 %>%
  count(condition, effectiveness, name = "n") %>%
  add_row(condition = "graph", effectiveness = 2, n = 0) %>%
  complete(condition, effectiveness, fill = list(n = 0)) %>%
  group_by(condition) %>%
  mutate(
    pr_k = n / sum(n),
    cum_pr_k = cumsum(pr_k),
    cum_pr_k_1 = cum_pr_k - (pr_k),
    logit_cum_pr_k = rethinking::logit(cum_pr_k))
)
```


```{r}
cumu_df1 %>%
  filter(!is.infinite(logit_cum_pr_k)) %>%
  ggplot(aes(x = effectiveness, y = logit_cum_pr_k, color = condition, label = round(logit_cum_pr_k, 2))) + 
  geom_vline(xintercept = 6) + 
  geom_label_repel(data = cumu_df1 %>% filter(effectiveness == 6)) + 
  geom_point() + 
  geom_line() + 
  scale_x_continuous(breaks = 1:9) + 
  ylab("log-cumulative-odds")
```
Once we have the estimates in the cumulative log odds space, can do inverse logit, and get proportions out of the cumulative proportions.

```{r}
cumu_df1 %>%
  ggplot(aes(x = effectiveness, y = cum_pr_k)) + 
  geom_line() +
  geom_segment(aes(
    x = effectiveness, 
    xend = effectiveness,
    y = cum_pr_k, 
    yend = cum_pr_k_1),
    color = "grey") +
  geom_segment(aes(
    x = effectiveness - 1, 
    xend = effectiveness,
    y = cum_pr_k_1, 
    yend = cum_pr_k_1),
    linetype = "dotted", color = "grey") +
  geom_point() +
  scale_x_continuous(breaks = 1:9) + 
  facet_grid(~condition)
```


```{r dat1-frequentist-model3}
(m_olr_freq <- df1 %>% 
  mutate(effectiveness = ordered(effectiveness, levels = as.character(1:9))) %>% 
  ordinal::clm(effectiveness ~ condition, data = .)
)
```


```{r}
glance(m_olr_freq)

tidy(m_olr_freq)
```

```{r}
(condition_no_graph <- tidy(m_olr_freq) %>% slice_tail())
```
Showing estimates on the cumulative log odds scale

```{r}
tidy(m_olr_freq) %>%
  dplyr::slice(-n()) %>%
  crossing(condition = c("graph", "no_graph")) %>%
  mutate(estimate = 
           ifelse(condition == "no_graph", 
                  estimate + condition_no_graph$estimate, 
                  estimate)) %>%
  mutate(.upper = estimate + 1.96 * std.error, 
         .lower = estimate - 1.96 * std.error)  %>%
  mutate(condition = ifelse(condition == "graph", 0, 1)) %>%
  ggplot(aes(x = condition, y = estimate, group = term)) + 
  facet_grid(~ term) + # EXERCISE: uncomment to see data better
  geom_lineribbon(aes(ymin = .lower, ymax = .upper, color = term), alpha = 0.4) + 
  scale_color_viridis_d()
```

Going back to the data space... show predicted proportions and uncertainty

```{r}
# EXERCISE: how to show uncertainty with this?

as.data.frame(predict(m_olr_freq, type.predict = "prob", se.fit = TRUE)) %>%
  bind_cols(df1 %>% select(effectiveness, condition))  %>%
  group_by(effectiveness, condition) %>% 
  slice_head() %>%
  ungroup()   %>%
  ggplot(aes(x = effectiveness, y = fit, fill = condition)) + 
  geom_col(position = "dodge") + 
  # geom_pointinterval(   # EXERCISE: put in the interval 
  #   aes(ymin = fit - 1.96 * se.fit, ymax = fit + 1.96 * se.fit),
  #   position = "dodge") + 
  scale_x_continuous(breaks = 1:9) + 
  ylab("Proportion")
```


## Dataset 1, Bayesian land


### Dataset 1, Bayesian t-test



```{r }
(m_t_test_bayesian <- readRDS("../02-bayesian_stats/rds/dataset1.brm.bayesiant.rds"))
```


```{r }
(grid_df1<- df1 %>%
  data_grid(condition, effectiveness) 
)

(means_df1_t <- grid_df1 %>%
  add_epred_draws(m_t_test_bayesian, newdata = . ,dpar = TRUE))

(preds_df1_t <- grid_df1 %>%
    add_predicted_draws(m_t_test_bayesian))
```

```{r}
df1 %>%
  ggplot(aes(x = effectiveness, y = condition)) + 
  stat_interval(aes(x = .prediction), data = preds_df1_t) +
  
  # EXERCISE: change to density plot
  stat_pointinterval(aes(x = .epred), data = means_df1_t,
                     .width = c(.66, .95),
                     position = position_nudge(y = -0.2)) +
  geom_jitter(height = 0.1) +
  scale_x_continuous(breaks = 1:9)+
  scale_color_brewer()
```


```{r}
means_df1_t %>%
  sample_draws(30) %>%
  ggplot(aes(y = condition)) + 
  stat_slab(aes(xdist = dist_student_t(nu, mu, sigma)),
                 slab_color="gray65", alpha = 0.05, fill = NA) +
  geom_jitter(aes(x = effectiveness), data = df1, shape = 21, fill = "#9ECAE1",  size = 2, width = 0.1, height = 0.1) +
  scale_x_continuous(breaks = 1:9)
  NULL
```

### Dataset 1, Bayesian OLR

There're starter values and stuff, not bothered just yet https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/monsters-and-mixtures.html#ordered-categorical-outcomes

```{r dat1-bayesian-model3}
(m_olr_bayes <- readRDS("../02-bayesian_stats/rds/dataset1.brm.olr1.rds"))
```


```{r}
(means_df1_olr <- expand(df1, condition) %>%
  add_epred_draws(m_olr_bayes) %>%
   rename(effectiveness = .category)
)
```


```{r}
(preds_df1_olr <- expand(df1, condition) %>%
    add_predicted_draws(m_olr_bayes) %>%
    select(-c(.chain, .iteration)) %>%
    drop_na()
 )
```
EXERCISE: why does this look different from the frequentist version?

```{r}
(df1_prop <- df1 %>%
  count(condition, effectiveness) %>%
  group_by(condition) %>%
  mutate(prop = n/sum(n))
)

means_df1_olr %>%
  ggplot(aes(x = condition, y = .epred)) + 
  stat_lineribbon()  + 
  geom_point(aes(x = condition, y = prop), data = df1_prop) + 
  scale_fill_brewer() + 
  facet_grid(~ effectiveness) +
  theme(axis.text.x = element_blank()) + 
  xlab("Condition: graph - no graph") + 
  ylab("Proportion")
```
```{r}
(means_df1_olr_compare <- means_df1_olr %>%
  group_by(effectiveness) %>%
  compare_levels(variable = .epred, by = condition ) 
)

(df1_prop_diff <- df1_prop %>%
  pivot_wider(id_col = effectiveness, names_from = condition, values_from = prop) %>%
  drop_na() %>%
  mutate(diff = no_graph - graph)
)

means_df1_olr_compare %>%
  ggplot(aes(x = effectiveness, y = .epred)) + 
  stat_interval(.width = c(0.95)) +  # EXERCISE: can you be more expressive with uncertainty?
  geom_hline(                        # EXERCISE: add reference line y = 0
    yintercept = 0, linetype = "dashed", color = "grey") + 
  geom_point(aes(y = diff), data= df1_prop_diff) +  #EXERCISE: add raw data
  scale_color_brewer() + 
  ylab("no graph - graph") + 
  labs(title = "Difference in proportions: no graph - graph") + 
  NULL
```


## Dataset 2 Power Pose

Load the dataframe. Data dictionary/column description at https://github.com/yvonnejansen/posture/blob/master/data/exp2_column-description.csv 

```{r}
df2 <- read_csv("../02-bayesian_stats/data/exp2.csv")
```

We only modeled two variables: `change` and `condition`. 

- Each of the 80 participants were assigned to either "constrictive" or "expansive" conditions. 
- `change` is "a normalized measure for how people’s behavior changed over the course of the experiment (∼10 min)". The experiment was done in three phases where participants pumped balloons. `change` $ = \frac{\text{mean adjusted # of pumps in Phase 3 - mean adjusted # of pumps in Phase 1}}{\text{mean adjusted # of pumps in Phase 3}}$


```{r}
df2 %>%
  group_by(condition) %>%
  count()
```

```{r}
df2 %>%
  ggplot(aes(x = change, color = condition)) +  
  # geom_dots( alpha = 0.4) + 
  geom_density(outline.type = "full") + 
  xlab("Percent change") + 
  NULL
```


## Dataset 2 results

```{r}
if (!file.exists("data/dataset2.brm.student.rds")){
fit.brm <- brm(
   bf(
    change ~ condition, sigma ~ condition, 
    family = student()
  ),
  prior = c(
    prior(normal(0, 2), class = "b"),
    prior(cauchy(0, 2), class = "b", dpar = "sigma"),
    prior(exponential(0.03), class = "nu"),
    prior(student_t(3, 0, 5), class = "Intercept"),
    prior(student_t(3, 0, 2), class = "Intercept", dpar = "sigma")
  ), 
  data = df2,
  seed = 99, chains = 4, cores = 4,
  file = "data/dataset2.brm.student.rds")
} else {
  fit.brm <- readRDS("data/dataset2.brm.student.rds")
}

fit.brm
get_variables(fit.brm)
```

```{r}
(grid <- df2 %>% data_grid(condition))

(means <- grid %>% 
  add_epred_draws(fit.brm, dpar = TRUE) 
  )

(preds <- grid %>% 
  add_predicted_draws(fit.brm)
)
```


```{r}
df2_range <- c(min(df2$change - 20), max(df2$change + 20))
```


```{r}
means %>%
  ggplot(aes(x = .epred, y = condition)) +
  stat_pointinterval(.width = 0)+ 
  xlim(df2_range)

means %>%
  ggplot(aes(x = .epred, y = condition)) +
  stat_pointinterval() + 
  xlim(df2_range)
```


```{r}
ggplot() + 
  stat_interval(
    aes(x = .prediction, y = condition),
    data = preds) + 
  geom_point(aes(x = change, y = condition), data = df2, alpha = .5) + 
  # stat_dots(
  #   aes(x = change, y = condition), 
  #   data = df2, alpha = 0.5, position = position_nudge(y = 0.1) , scale = 0.8) + 
  stat_pointinterval(
    aes(x = .epred, y = condition), 
    data = means, position = position_nudge(y = -0.1)) +
  scale_color_brewer()

```

(Rethinking: think about what these all mean)

- How wide are the intervals?


```{r}
ggplot(mapping = aes(fill = condition)) + 
  stat_slab(
    aes(x = .prediction, y = condition, thickness = stat(pdf * n)),
    data = preds, scale = 0.5) + 
  stat_dots(
    aes(x = change, y = condition, side = "bottom"), 
    data = df2, alpha = 0.5, scale = 0.7) + 
  stat_pointinterval(
    aes(x = .epred, y = condition), 
    data = means) + 
  labs(title = "Rainplot") +
  xlim(df2_range)
```


```{r}
means %>%
  sample_draws(30) %>%
  ggplot(aes(y = condition)) + 
  stat_slab(aes(xdist = dist_student_t(nu, mu, sigma)),
                 slab_color="gray65", alpha = 0.1, fill = NA) +
  geom_point(aes(x = change), data = df2, shape = 21, fill = "#9ECAE1",  size = 2) +
  xlim(df2_range) + 
  NULL
```

