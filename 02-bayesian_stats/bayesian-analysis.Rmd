---
title: "Introduction to Transparent Bayesian Data Analysis (chi22-course)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
author: Abhraneel Sarma and Fumeng Yang
output: 
  html_document: 
    highlight: tango
    toc: yes
    df_print: kable
---

<style>
.sourceCode.r{
  background-color:#f7f8f9ff;
  font-family: Courier New;
  font-weight: bold
}


</style>

```{r knit-github, eval=FALSE,echo=FALSE,include=FALSE}
# compile for github
library(rmarkdown)
render("02-bayesian_stats/bayesian-analysis.Rmd", md_document(variant = "gfm"), knit_root_dir = "/Users/fm/Documents/Github/CHI-Course-Transparent-Quant")
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(dplyr)
library(tibble)
library(purrr)
library(tidyr)
library(forcats)

library(broom)
library(broom.mixed)

library(modelr)

library(brms)
library(tidybayes)

library(ggdist)
library(bayesplot)
library(ggplot2)
library(knitr)

theme_set(theme_ggdist() + 
          theme(strip.background = element_blank(),
               plot.title = element_text(vjust = .5)))

IF_RUN_MODEL <- T

set.seed(99) 

DARK_PURPLE <- '#351c75'
```


# Introduction

In this document, we will outline the Bayesian analogs of the statistical analyses described [here](https://github.com/chatchavan/CHI-Course-Transparent-Quant/blob/master/frequentist.Rmd) (and in the previous course lecture).


Helper functions.

```{r helper}

# this function extracts results from different models and generate results of the same format to be used in visualizations
tidy.wrapper = function(model) {
  if (class(model) == "lm") {
    tidy(model, conf.int = TRUE) %>%
      select(-c(statistic, p.value)) %>%
      mutate(model = "Frequentist") %>%
      select(model, everything())
  } else if (class(model) == "brmsfit") {
    tidy(model) %>%
      filter(effect == "fixed") %>%
      select(c(term:conf.high)) %>%
      mutate(model = "Bayesian") %>%
      select(model, everything())
  } else {
    stop("unknown model class")
  }
}
```


# Dataset 1

Let's first load and look at the data in a table

```{r load-dataset}
dataset = readr::read_csv("02-bayesian_stats/data/blinded.csv")
head(dataset)
```  


```{r plot-dataset, fig.height = 3, fig.width = 7}
dataset %>% 
  mutate(effectiveness = fct_rev(factor(effectiveness, levels = 1:9)),
         experiment = as.factor(experiment)) %>%
  
  # stacked bar plot
  ggplot(aes(x = condition, fill = effectiveness)) +
  geom_bar(position = "stack", stat="count") +
  
  # plot data for different experiments as small multiples
  facet_wrap(. ~ experiment) +
  
  # grey color scale is robust for colorblind
  scale_fill_brewer(palette="Purples", drop = FALSE) +
  
  # horizontal plot
  coord_flip() +
  
  # legend
  guides(fill = guide_legend(reverse = TRUE)) 
```

As we can see above, the original dataset contains results from four different experiments. For the purposes of this lecture, we will confine ourselves to the first experiment.

```{r filter-data}
exp1.data = dataset %>%
  filter(experiment == 1)

head(exp1.data)
```


## Model 1. Wilcoxon signed rank test

This is a non-parametric test which we will skip for now. Although, there exists Bayesian non-parametric methods, they are more advanced for this lecture.

## Model 2. Bayesian t-test

This is the linear model equivalent for the paired sample t-test.

```{r dataset1-lm}
dataset1.lm.freqt <-
  lm(
    effectiveness ~ condition - 1, # we remove intercept, which is equivalent 
    data = exp1.data
  )
```


### Step 1: Build the model (likelihood)


Although we know that we are going to use a t distribution, we still plot the data to set how it looks like. Usually, this will help us decide our likelihood function.

```{r dataset1-distribution}
exp1.data %>%
  ggplot(., aes(x = effectiveness)) +
  geom_histogram(fill = '#351c75',
                 color = NA,
                 binwidth = .5)

```
Then we write the formula using brm formula (`bf`) function.

```{r dataset1-bayesian-t-test-formula}
dataset1.brm.bayesiant.formula <- bf(effectiveness ~ condition - 1,
                                     family = student())


```

### Step 1: Build the model (priors)

We check the priors that we should set

The blank ones are flat (uniform) priors. These are improper priors and usually needed to be set.
We can and should adjust other priors given by `brm`.

```{r dataset1-bayesian-t-test-get-prior}

as_tibble(get_prior(dataset1.brm.bayesiant.formula, data = exp1.data))

```
Our priors. 

```{r dataset1-bayesian-t-test-priors}
dataset1.brm.bayesiant.priors = c(
  # there's a lot of data so even fairly "strong" priors are going to not matter so much here
  prior(normal(0, 1), class = "b"),
  prior(student_t(3, 0, 1), class = "sigma"))
```


### Step 1: Build the model (prior predictive checks)

We do a prior predictive checks to see the predicted range if we only use priors.
This helps us understand the predicted range of priors and formula, and also helps us understand if our priors and formula are appropriate. 

Actually, the predicted range is quite wide for this model. We could consider shrink priors. 

```{r dataset1-bayesian-t-test-run-prior-predictive-check, fig.height=4, fig.width=9}
dataset1.brm.bayesiant.priorchecks <-
  brm(
    dataset1.brm.bayesiant.formula,
    prior = dataset1.brm.bayesiant.priors,
    data = exp1.data,
    backend = "cmdstanr",
    sample_prior = "only",
    file = "02-bayesian_stats/rds/dataset1.brm.bayesiant.priorchecks.rds"
  )


dataset1.bayesiant.yprior <-
  posterior_predict(dataset1.brm.bayesiant.priorchecks)

ggplot() +
  geom_density(
    aes(x = dataset1.bayesiant.yprior),
    color = '#351c75',
    alpha = .5,
    size = 1
  ) +
  xlab('prior draws')
```

### Step 2: Computing the posterior probability

```{r dataset1-bayesian-t-test-run}
dataset1.brm.bayesiant =
  brm(
    dataset1.brm.bayesiant.formula,
    prior = dataset1.brm.bayesiant.priors,
    data = exp1.data,
    # You may not need this if rstan works with your computer.
    backend = "cmdstanr",
    # There are many other parameters you can play with.
    # Here we use default parameters to simplify the lecture.
    # We save the model 
    # But if you have this file, brm only loads the model for this file
    #file = "02-bayesian_stats/rds/dataset1.brm.bayesiant.rds"
  )

```


### Step 3: Evaluate the fit



```{r dataset1-bayesian-t-summary}
summary(dataset1.brm.bayesiant)
```

### Step 3: Evaluate posterior predictions

We then call `bayesplot` to draw posterior distributions and MCMC traces for each parameter.

We should see that the four chairs are mixed well, meaning that the model converged.

```{r dataset1-diagnotics-1}
color_scheme_set(scheme = "purple")
plot(dataset1.brm.bayesiant)
```

### Step 3: Evaluate posterior predictions


We draw from posterior predictive distributions. They are many ways you can do this. Here we use posterior_predict from `brms`. 


```{r dataset1-posterior-diagnotics-2}
dataset1.bayesiant.y <- exp1.data$effectiveness
dataset1.bayesiant.yrep <-
  posterior_predict(dataset1.brm.bayesiant)

head(as_tibble(dataset1.bayesiant.yrep[, 1:11]))         
```

We use bayesplot to plot the first eight draws of posterior predictions and the original data. Again, there are many ways to do this (more in Lecture 3).

```{r dataset1-posterior-diagnotics-3, fig.height=3, fig.width=10}
ppc_hist(y = dataset1.bayesiant.y,
         yrep = dataset1.bayesiant.yrep[1:8,],
         binwidth = .5)

ppc_dens_overlay(y = dataset1.bayesiant.y,
                 yrep = dataset1.bayesiant.yrep[1:30,])

ppc_stat_grouped(y = dataset1.bayesiant.y,
                 yrep = dataset1.bayesiant.yrep,
                 group = exp1.data$condition)
  
  
```


### Step 3: Compare posterior predictions

We compare the two conditions using conditional distributions. add_epred_draws from tidybayes is the most convenient wayâ€¦ 


```{r dataset1-t-test-posterior-fitted}
dataset1.bayesiant.posterior_epred <-
  tibble(condition = c('graph', 'no_graph')) %>%
  add_epred_draws(dataset1.brm.bayesiant,
                  re_formula = NA,
                  allow_new_levels = FALSE) %>%
  ungroup()

kable(head(dataset1.bayesiant.posterior_epred))
```
We then transform the data, subtract the two conditions from each other, and calculate the credible intervals.


```{r dataset1-bayesiant-posterior_comparison}
dataset1.bayesiant.posterior_comparison <-
  dataset1.bayesiant.posterior_epred %>%
  select(-c(.chain, .iteration, .row)) %>%
  compare_levels(variable = .epred, by = condition)

# just use head(dataset1.bayesiant.posterior_comparison) if you don't need to knit n html
head(dataset1.bayesiant.posterior_comparison)


dataset1.bayesiant.posterior_comparison %>%
        mean_qi(.epred)
```
We can also plot the credible interval. Lecture 3 will cover more. 

```{r dataset1.bayesiant.diffci, fig.height=1, fig.width=5}
dataset1.bayesiant.posterior_comparison %>%
  median_qi(.epred) %>%
  ggplot() +
  geom_point(aes(x = .epred, y = condition), color = DARK_PURPLE, size = 3) +
  geom_errorbarh(
    aes(xmin = .lower, xmax = .upper, y = condition),
    color = DARK_PURPLE,
    alpha = .5,
    size = 2,
    height = 0
  ) +
  geom_vline(aes(xintercept = 0), linetype = "dashed", color = "gray") +
  coord_cartesian(ylim = c(0, 2), xlim = c(-1, 1))  +
  xlab('') + ylab('')
```

### Compared to the frequentist estimates

```{r dataset1-t-test-compare, fig.height=4, fig.width=10, warning=FALSE}
bind_rows(tidy.wrapper(dataset1.lm.freqt),
          tidy.wrapper(dataset1.brm.bayesiant)) %>%
  ggplot() +
  geom_pointrange(
    aes(
      x = model,
      y = estimate,
      ymin = conf.low,
      ymax = conf.high,
      color = term
    ),
    position = position_dodge(width = 0.2)
  ) +
  scale_color_brewer(palette = "Set1") +
  ylab('effectiveness') +
  scale_y_continuous(breaks = 1:9, limits = c(1, 9)) +
  coord_flip()
```


## Model 3. Ordinal Logistic Regression

A more appropriate to analyze ordianl data is to use ordinal logistic regression.
In this section, we reanalyze the data using Bayesian rdinal logistic regression.

### Step 1: Build the model (likelihood) 

```{r bayesian-ordinal-logistic-regression-formula}
dataset1.brm.olr.formula <-
  bf(effectiveness ~ condition,
     family = cumulative("logit"))
```


### Step 1: Build the model (priors)

```{r bayesian-ordinal-logistic-regression-prior}
as_tibble(get_prior(dataset1.brm.olr.formula, data = exp1.data))

dataset1.brm.olr.priors = c(prior(normal(0, 1), class = "b"),
                            prior(student_t(3, 0, 1), class = "Intercept"))

```


### Step 1: Build the model (prior predictive checks) 

TODO We need to fix this..

```{r bayesian-ordinal-logistic-regression-prior-checks, fig.height=4, fig.width=8}

dataset1.brm.olr.priorchecks <- brm(
    effectiveness ~ condition,
    prior = dataset1.brm.olr.priors,
    data = exp1.data,
    family= cumulative("logit"),
    backend = 'cmdstanr',
    sample_prior = 'only',
    file = "02-bayesian_stats/rds/dataset1.brm.bayesiant.priorchecks.rds"
    )


dataset1.olr.yprior <-
  posterior_predict(dataset1.brm.olr.priorchecks)


ggplot() +
  geom_histogram(aes(x = dataset1.olr.yprior),
               fill = '#351c75',
               alpha = .5,
               size = 1,
               binwidth = .5) +
  scale_x_continuous(breaks = 1:9, limits = c(1,9)) + 
  xlab('prior draws') + ylab('')
```

### Step 2: Computing the posterior probability

```{r bayesian-ordinal-logistic-regression-run-1}
dataset1.brm.olr1 =
  brm(
    dataset1.brm.olr.formula,
    prior = dataset1.brm.olr.priors,
    data = exp1.data,
    backend = "cmdstanr",
    file = "02-bayesian_stats/rds/dataset1.brm.olr1.rds"
  )

summary(dataset1.brm.olr1)

```

We see the error messages. Here we adjust a few sampling parameters to help the model converge.

```{r bayesian-ordinal-logistic-regression-run-2}

dataset1.brm.olr2 =
  brm(
    dataset1.brm.olr.formula,
    prior = dataset1.brm.olr.priors,
    data = exp1.data,
    backend = "cmdstanr",
    warmup = 1500,
    iter = 2500,
    control = list(adapt_delta = 0.99, max_treedepth = 15),
    file = "02-bayesian_stats/rds/dataset1.brm.olr2.rds"
  )

summary(dataset1.brm.olr2)

```

### Step 3: Evaluate the fit (MCMC traces)


```{r dataset1-brm-olr-mcmc}
plot(dataset1.brm.olr2)
```

### Step 3: Evaluate posterior predictions 


```{r dataset1-olr-posterior-checks, fig.height=3, fig.width=10}
dataset1.olr.y <- exp1.data$effectiveness
dataset1.olr.yrep <- posterior_predict(dataset1.brm.olr2)

ppc_hist(y = dataset1.olr.y,
         yrep = dataset1.olr.yrep[1000:1007, ],
         binwidth = .5)

ppc_dens_overlay(y = dataset1.olr.y,
                 yrep = dataset1.olr.yrep[2000:2030, ])


ppc_stat_grouped(y = dataset1.olr.y,
                 yrep = dataset1.olr.yrep,
                 group = exp1.data$condition)

```
### Step 3: Compare posterior predictions

```{r dataset1-olr-epred-draws}

dataset1.olr.posterior_epred <-
  tibble(condition = c('graph', 'no_graph')) %>%
  add_epred_draws(dataset1.brm.olr2,
                  re_formula = NA,
                  allow_new_levels = FALSE) %>%
  ungroup()

head(dataset1.olr.posterior_epred)

dataset1.olr.posterior_comparison <-
  dataset1.olr.posterior_epred %>%
  select(-c(.chain, .iteration, .row)) %>%
  group_by(.category) %>%
  compare_levels(variable = .epred, by = condition)

head(dataset1.olr.posterior_comparison %>%
       mean_qi())
```

```{r dataset1-olr-comparison, fig.height=3, fig.width=10}
dataset1.olr.posterior_comparison %>%
  mean_qi(.epred) %>%
  ggplot() +
  geom_point(aes(y = .epred, x = .category), size = 3, color = DARK_PURPLE) +
  geom_errorbar(
    aes(ymin = .lower, ymax = .upper, x = .category),
    width = 0,
    size = 2,
    color = DARK_PURPLE,
    alpha = .5
  ) +
  geom_hline(aes(yintercept = 0), linetype = "dashed", color = "gray") +
  xlab('') + ylab('no_graph - graph') +
  theme(axis.title.y = element_text(angle = 0, vjust = .5))
```

# Dataset 2

Load the data.

```{r load-dataset-2}
dataset2 = readr::read_csv("02-bayesian_stats/data/exp2.csv") %>%
  mutate(condition = condition == 'expansive') %>%
  group_by(participant)

head(dataset2)
```
We plot the data to give us a better picture about its distribution.

```{r dataset2, fig.height=3, fig.width=7}
dataset2 %>%
  mutate(c = as.factor(condition)) %>%
  ggplot(aes(x = change)) +
  geom_histogram(
    aes(y = ..density..),
    binwidth = 6,
    fill = DARK_PURPLE,
    color = 'white'
  ) +
  geom_density(size = 1,
               adjust = 1.5,
               color = 'lightgray') +
  scale_x_continuous(limits = c(-200, 200)) 
```

### Step 1: Build the model (likelihood and priors)

This model is the BEST test model as described by Kruschke in the paper *Bayesian estimation supersedes the t-test*. In this model, $\beta$ indicates the mean difference in the outcome variable between the two groups (in this case, the percent change in the BART scores). We fit different priors on $\beta$ and set different weights on these priors to obtain our posterior estimate.

$$
\begin{align}
y_{i} &\sim \mathrm{T}(\nu, \mu, \sigma) \\
\mu &= \alpha_{0} + \beta * x_i \\
\sigma &= \sigma_{a} + \sigma_{b}*x_i \\
\beta &\sim \mathrm{N}(\mu_{0}, \sigma_{0}) \\
\sigma_a, \sigma_b &\sim \mathrm{Cauchy}(0, 2) \\
\nu &\sim \mathrm{exp}(1/30)
\end{align}
$$

We translate the above specification to R code.

```{r dataset2-model1-formula}
dataset2.brm.student.formula <- bf(change ~ condition,
                                   sigma ~ condition,
                                   family = student())

head(as_tibble(get_prior(dataset2.brm.student.formula, data = dataset2)))

```


```{r dataset2-bayesiant-prior-checkes}
dataset2.brm.student.priorchecks = brm(
  dataset2.brm.student.formula,
  prior = c(
    prior(normal(0, 2), class = "b"),
    prior(cauchy(0, 2), class = "b", dpar = "sigma"),
    prior(exponential(0.033), class = "nu"),
    prior(student_t(3, 0, 5), class = "Intercept"),
    prior(student_t(3, 0, 2), class = "Intercept", dpar = "sigma")
  ),
  data = dataset2,
  backend = "cmdstanr",
  sample_prior = 'only',
  file = "02-bayesian_stats/rds/dataset2.brm.student.priorchecks.rds"
)
# This breaks :-)
# dataset2.student.yprior <-
#   posterior_predict(dataset2.brm.student.priorchecks)

# ggplot() +
#   geom_density(
#     aes(x = dataset2.student.yprior),
#     color = '#351c75',
#     alpha = .5,
#     size = 1
#   ) +
#   xlab('prior draws') +
#   ggtitle('prior preditive check')

```


### Step 2: Computing the posterior probability

```{r dataset2-bayesiant}
dataset2.brm.student = brm(
 bf(change ~ condition, 
 sigma ~ condition,
 family = student()),
 prior = c(
   prior(normal(0, 2), class = "b"),
   prior(cauchy(0, 2), class = "b", dpar = "sigma"),
   prior(exponential(.033), class = "nu"),
   prior(student_t(3, 0, 5), class = "Intercept"),
   prior(student_t(3, 0, 2), class = "Intercept", dpar = "sigma")
 ), 
 data = dataset2,
 backend = "cmdstanr",
 file = "02-bayesian_stats/rds/dataset2.brm.student.rds"
)

dataset2.brm.student$prior
```

### Step 3: Evaluate the fit (summary)

```{r dataset2-brm-student-summary}
summary(dataset2.brm.student)
```

### Step 3: Evaluate the posterior predictions 

```{r dataset2-diagnotics-1, fig.height=8, fig.width=10}
plot(dataset2.brm.student)
```

```{r dataset2-posterior-diagnotics-2}

dataset2.student.y <- dataset2.brm.student$data$change
dataset2.student.yrep <- posterior_predict(dataset2.brm.student)

dataset2.student.epred <- tibble(condition = c(TRUE, FALSE)) %>%
  add_epred_draws(dataset2.brm.student,
                  re_formula = NA,
                  allow_new_levels = TRUE) %>%
  ungroup()
```



```{r dataset2-posterior-diagnotics-3, fig.height=3, fig.width=10}
ppc_hist(y = dataset2.student.y,
         yrep = dataset2.student.yrep[100:107, ],
         binwidth = 10)


ppc_dens_overlay(y = dataset2.student.y,
                 yrep = dataset2.student.yrep[2000:2030, ])


ppc_stat_grouped(
  y = dataset2.student.y,
  yrep = dataset2.student.yrep,
  group = dataset2$condition,
  binwidth = 5
)
```

### Step 3: Compare the posterior predictions 

```{r dataset2-student-posterior-fitted}

dataset2.student.epred_comparison <-
  tibble(condition = c(TRUE, FALSE)) %>%
  add_epred_draws(dataset2.brm.student,
                  re_formula = NA,
                  allow_new_levels = FALSE) %>%
  ungroup() %>%
  select(-c(.chain, .iteration, .row)) %>%
  compare_levels(variable = .epred, by = condition) %>%
  rename(diff = .epred)


kable(head(dataset2.student.epred_comparison))


```

```{r dataset2.studenyt.diffci, fig.height=1, fig.width=5}
dataset2.student.epred_comparison %>%
  select(diff) %>%
  mean_qi() %>%
  ggplot() +
  geom_point(aes(x = diff, y = condition), size = 3, color = DARK_PURPLE) +
  geom_errorbarh(
    aes(xmin = .lower, xmax = .upper, y = condition),
    height = 0,
    color = DARK_PURPLE,
    alpha = .5,
    size = 2
  ) +
  geom_vline(aes(xintercept = 0), linetype = "dashed", color = "gray") +
  coord_cartesian(ylim = c(0, 2), xlim = c(-5, 5))  +
  scale_y_discrete(label = c('expansive - not expansive')) +
  xlab('')  + ylab('')
```

