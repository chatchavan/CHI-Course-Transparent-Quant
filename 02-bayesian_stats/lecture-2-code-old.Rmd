---
title: "Introduction to Transparent Bayesian Data Analysis (chi2-course)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
author: Abhraneel Sarma and Fumeng Yang

output: 
  html_document: 
    highlight: pygments
    toc: yes
    toc_float: true
    toc_collapsed: true
    toc_depth: 3
    df_print: kable
---

```{=html}
<style>
.sourceCode.r{
  background-color:#f7f8f9ff;
  font-family: Courier New;
  font-weight: bold
}

</style>
```
```{r knit-github, eval=FALSE,echo=FALSE,include=FALSE}
# compile for github, you can ignore this
library(rmarkdown)
render("bayesian-analysis.Rmd", md_document(variant = "gfm"), knit_root_dir = "/Users/fm/Documents/Github/CHI-Course-Transparent-Quant")
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
rm(list = ls(all.names = TRUE)) 

library(dplyr)
library(tibble)
library(purrr)
# library(ggpubr)
library(tidyr)
library(forcats)
library(gtools)

library(broom)
library(broom.mixed)

library(modelr)

library(brms)
library(tidybayes)

library(ggdist)
library(bayesplot)
library(ggplot2)
library(knitr)

# set up the global theme
theme_set(theme_ggdist() + 
          theme(strip.background = element_blank(),
               plot.title = element_text(hjust = .5)))

set.seed(99) 

BRM_BACKEND <- ifelse(require("cmdstanr"), 'cmdstanr', 'rstan')

DARK_PURPLE <- '#351c75'
GOLD_COLOR <- '#8f5902'

COLOR_PALETTE <- c(DARK_PURPLE, GOLD_COLOR)
```

# Introduction

In this document, we will outline the Bayesian analogs of the statistical analyses described in lecture 1 ([Github code](https://github.com/chatchavan/CHI-Course-Transparent-Quant-Lecture-1)).

## Note on MathJax

If you use RStudio's Visual editor mode. Check if you can see anything written in the following parenthesis: ($k$). If not, your RStudio has a bug. This problem will be fixed in future versions. Meanwhile, if some text seems to be missing in the file below, simply position your cursor around the missing area to reveal the underlying code.

## Helper function

The following function extracts results from different models and generate results of the same format to be used in visualizations

```{r helper}
tidy.wrapper = function(model) {
  if (class(model) == "lm") {
    tidy(model, conf.int = TRUE) %>%
      select(-c(statistic, p.value)) %>%
      mutate(model = "Frequentist") %>%
      select(model, everything())
  } else if (class(model) == "brmsfit") {
    tidy(model) %>%
      filter(effect == "fixed") %>%
      select(c(term:conf.high)) %>%
      mutate(model = "Bayesian") %>%
      select(model, everything())
  } else {
    stop("unknown model class")
  }
}
```

# Dataset 2

The following dataset is from experiment 2 of "How Relevant are Incidental Power Poses for HCI?" [(Jansen & HornbÃ¦k, 2018)](https://doi.org/10.1145/3173574.3173588). Study participants were asked to either make an expansive posture or a constrictive posture before performing a task. The experiment investigated whether posture could potentially have an effect on risk taking behavior.

First, we load the data.

```{r load-dataset-2}
dataset2 = readr::read_csv("data/poses_data.csv", show_col_types = FALSE) %>%
  mutate(condition = condition == 'expansive') %>%
  mutate(c = ifelse(condition, "Expansive", "Constrictive")) %>%
  group_by(participant)

head(dataset2)
```

The data has been aggregated for each participant: - `condition = TRUE` indicates expansive posture, and `condition = FALSE` indicates constrictive posture - The dependent variable is `change` which indicates the percentage change in risk-taking behavior. Thus, it is a continuous variable.

For the purposes of this demo, we are only concerned with these two variables. We can ignore the other variables for now.

We plot the data to give us a better picture about its distribution.

```{r dataset2, fig.height=3, fig.width=7}
dataset2 %>%
  mutate(c = as.factor(condition)) %>%
  ggplot(aes(x = change)) +
  geom_histogram(
    aes(y = ..density..),
    binwidth = 10,
    fill = DARK_PURPLE,
    alpha = .5,
    color = 'white'
  ) +
  geom_density(size = 1,
               adjust = 1.5,
               color = '#9281bf') +
   geom_function(
    color = "#222222",
    linetype = 'dashed',
    fun = function(x)
      dstudent_t(x, mu = 16,  sigma = 39, df = 6),
    size = 1
  ) + 
  scale_x_continuous(limits = c(-200, 200)) 
```

## Intuition of Bayesian Posteriors

The Bayesian t-test (BEST) assumes that the data in the two conditions arises from two separate t-distributions. In the following section, we will describe the process for one of the conditions in the experiment.

We will use the $Normal(\mu = 20, \sigma = 20)$ as the prior distribution.

First, we define some functions for manual calculation of the posterior normal distribution:

```{r}
sigma_post = function(sigma_prior, sigma, n = 1) {
  sqrt(1 / (1 / (sigma_prior^2) + n / (sigma^2)))
}

mu_post = function(mu_prior, sigma_prior, mu, sigma, n = 1) {
  tau = sigma_post(sigma_prior, sigma, n)
  (tau^2 / sigma_prior^2)*mu_prior + (n * tau^2 / sigma^2)*mu
}
```

```{r}
d.p2 = tibble(
  group = c("Prior", "Expansive", "Constrictive", "Posterior"), 
  mu = c(20, 32.82, 31.61, mu_post(20, 20, 32.82, 7.52)), 
  sd = c(20, 7.52, 7.06, sigma_post(20, 7.52))
) %>%
  mutate(
    cutoff_group = list(c(1:7)),
    cutoff = list(c(0, 15, 25, 30, 32.82, 40, 100))
  ) %>%
  unnest(c(cutoff_group, cutoff)) %>%
  mutate(
    x = map(cutoff, ~ seq(from = -40, ., by = 0.1)),
    y = pmap(list(x, mu, sd), ~ dnorm(..1, ..2, ..3))
  )
```

We also define some ggplot styles:

```{r}
theme_density = theme(
    axis.text.y = element_text(colour = "#ffffff"),
    axis.title.y = element_blank(),
    axis.line.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.x = element_text(colour = "#ffffff"),
    axis.title.x = element_blank(),
    axis.line.x = element_blank(),
    axis.ticks.x = element_blank()
  )
```

In the plot below, we show the raw data distribution for the two conditions:

```{r, fig.height = 4, fig.width = 9}
p1 = dataset2 %>%
  ggplot() +
  geom_point( aes(x = change, y = c, colour = c), position = position_jitter(height = 0.1), alpha = 0.7) +
  scale_colour_manual(values = COLOR_PALETTE) + 
  labs(y = "Condition") +
  theme(
    legend.position = "none", 
    axis.line.y = element_blank(), 
    axis.ticks.y = element_blank(),
    axis.title.y = element_blank()
  ) +
  scale_x_continuous(limits = c(-40, 100), breaks = seq(-40, 100, by = 20))

p2.blank = tibble(y = c("Expansive", "Constrictive"), x = 0) %>%
  ggplot(aes(x, y)) +
  theme_density

cowplot::plot_grid(p2.blank, p1, nrow = 2)
```

Next, we plot the prior density:

```{r, fig.height = 4, fig.width = 9}
p2.prior = d.p2 %>%
  filter(cutoff_group == 7 & group == "Prior") %>%
  unnest(c(x, y)) %>%
  ggplot(aes(x, y)) +
  geom_line(color = "red", size = 1) +
  geom_area(fill = "red", size = 1, alpha = 0.5) +
  scale_x_continuous(limits = c(-40, 100)) +
  scale_y_continuous(limits = c(0, 0.1)) +
  theme_density

cowplot::plot_grid(p2.prior, p1, nrow = 2)
```

Then we describe step by step, how the likelihood is computed:

```{r, fig.height = 4, fig.width = 9}
p2.ll.0 = d.p2 %>%
  filter(cutoff_group == 1 & group == "Expansive") %>%
  unnest(c(x, y)) %>%
  ggplot(aes(x, y)) +
  geom_line(color = "purple", size = 1) +
  geom_area(fill = "purple", size = 1, alpha = 0.5) +
  scale_x_continuous(limits = c(-40, 100)) +
  scale_y_continuous(limits = c(0, 0.1)) +
  theme_density

cowplot::plot_grid(p2.ll.0, p1, nrow = 2)
```

```{r, fig.height = 4, fig.width = 9}
p2.ll.1 = d.p2 %>%
  filter(cutoff_group == 2 & group == "Expansive") %>%
  unnest(c(x, y)) %>%
  ggplot(aes(x, y)) +
  geom_line(color = "purple", size = 1) +
  geom_area(fill = "purple", size = 1, alpha = 0.5) +
  scale_x_continuous(limits = c(-40, 100)) +
  scale_y_continuous(limits = c(0, 0.1)) +
  theme_density

cowplot::plot_grid(p2.ll.1, p1, nrow = 2)
```

```{r, fig.height = 4, fig.width = 9}
p2.ll.2 = d.p2 %>%
  filter(cutoff_group == 3 & group == "Expansive") %>%
  unnest(c(x, y)) %>%
  ggplot(aes(x, y)) +
  geom_line(color = "purple", size = 1) +
  geom_area(fill = "purple", size = 1, alpha = 0.5) +
  scale_x_continuous(limits = c(-40, 100)) +
  scale_y_continuous(limits = c(0, 0.1)) +
  theme_density

cowplot::plot_grid(p2.ll.2, p1, nrow = 2)
```

```{r, fig.height = 4, fig.width = 9}
p2.ll.3 = d.p2 %>%
  filter(cutoff_group == 4 & group == "Expansive") %>%
  unnest(c(x, y)) %>%
  ggplot(aes(x, y)) +
  geom_line(color = "purple", size = 1) +
  geom_area(fill = "purple", size = 1, alpha = 0.5) +
  scale_x_continuous(limits = c(-40, 100)) +
  scale_y_continuous(limits = c(0, 0.1)) +
  theme_density

cowplot::plot_grid(p2.ll.3, p1, nrow = 2)
```

```{r, fig.height = 4, fig.width = 9}
p2.ll.5 = d.p2 %>%
  filter(cutoff_group == 5 & group == "Expansive") %>%
  unnest(c(x, y)) %>%
  ggplot(aes(x, y)) +
  geom_line(color = "purple", size = 1) +
  geom_area(fill = "purple", size = 1, alpha = 0.5) +
  scale_x_continuous(limits = c(-40, 100)) +
  scale_y_continuous(limits = c(0, 0.1)) +
  theme_density

cowplot::plot_grid(p2.ll.5, p1, nrow = 2)
```

```{r, fig.height = 4, fig.width = 9}
p2.ll.6 = d.p2 %>%
  filter(cutoff_group == 6 & group == "Expansive") %>%
  unnest(c(x, y)) %>%
  ggplot(aes(x, y)) +
  geom_line(color = "purple", size = 1) +
  geom_area(fill = "purple", size = 1, alpha = 0.5) +
  scale_x_continuous(limits = c(-40, 100)) +
  scale_y_continuous(limits = c(0, 0.1)) +
  theme_density

cowplot::plot_grid(p2.ll.6, p1, nrow = 2)
```

```{r, fig.height = 4, fig.width = 9}
p2.ll.7 = d.p2 %>%
  filter(cutoff_group == 7 & group == "Expansive") %>%
  unnest(c(x, y)) %>%
  ggplot(aes(x, y)) +
  geom_line(color = "purple", size = 1) +
  geom_area(fill = "purple", size = 1, alpha = 0.5) +
  scale_x_continuous(limits = c(-40, 100)) +
  scale_y_continuous(limits = c(0, 0.1)) +
  theme_density

cowplot::plot_grid(p2.ll.7, p1, nrow = 2)
```

We want to compute the posterior, which is the product of the prior and likelihood:

```{r, fig.height = 4, fig.width = 9}
p2.prior_ll = d.p2 %>%
  filter(cutoff_group == 7 & group %in% c("Prior", "Expansive")) %>%
  unnest(c(x, y)) %>%
  ggplot() +
  geom_line(aes(x, y, color = group)) +
  geom_area(aes(x, y, fill = group), position = "identity", alpha = 0.5) +
  scale_x_continuous(limits = c(-40, 100)) +
  scale_y_continuous(limits = c(0, 0.1)) +
  theme_density

cowplot::plot_grid(p2.prior_ll, p1, nrow = 2)
```

```{r, fig.height = 4, fig.width = 9}
p2.prior_ll_post = d.p2 %>%
  filter(cutoff_group == 7 & group != "Constrictive") %>%
  unnest(c(x, y)) %>%
  ggplot() +
  geom_line(aes(x, y, color = group)) +
  geom_area(aes(x, y, fill = group), position = "identity", alpha = 0.5) +
  scale_x_continuous(limits = c(-40, 100)) +
  scale_y_continuous(limits = c(0, 0.1)) +
  theme_density

cowplot::plot_grid(p2.prior_ll_post, p1, nrow = 2)
```

## Model 1. Wilcoxon signed rank test

The first model discussed in the previous lecture was the `Wilcoxon signed rank test`. This is a non-parametric test which we will skip for now. Although, there exists Bayesian non-parametric methods, they are too advanced for this lecture.

## Model 2. Student's *t*-test

The second model discussed was the *t*-test. Although R contains a function (`t.test`) to perform this analysis, the *t*-test is essentially a linear regression, and thus can be performed using the linear model function in R (`lm`). The following code shows the equivalent linear-model for a paired sample *t*-test:

### Frequentist t-test

```{r dataset1-lm}
dataset1.lm.freqt <-
  lm(
    effectiveness ~ condition - 1, # we remove intercept
    data = exp1.data
  )
```

### Step 1: Build the model (likelihood)

Before we implement a Bayesian model, let us take a look at the distribution of responses. Although we know that we are going to use a *t* distribution, we still plot the data to set how it looks like. Usually, this will help us decide our likelihood function.

```{r dataset1-distribution}
  exp1.data %>%
     ggplot(., aes(x = effectiveness)) +
  geom_histogram(fill = DARK_PURPLE, color = NA, binwidth = 0.5, center = 0) +
  scale_x_continuous(breaks = seq(0, 10, by = 1))
```

We can see from the above plot that the responses are discrete. This is important to keep in mind when specifying a Bayesian model.

Let's also look how a Student's *t* distribution varies different parameter. Mu is location, sigma is dispersion (which is similar to the standard deviation parameter of the Gaussian distribution), and nu (df) control the tail.

```{r draw-student-t, fig.height=4, fig.width=8}
ggplot() +  xlim(-10, 10) +
  geom_function(
    color = "black",
    fun = function(x)
      dstudent_t(x, mu = 0, sigma = 1, df = 5) ,
    size = 1
  ) +
  geom_function(
    color = "gray80",
    fun = function(x)
      dstudent_t(x, mu = 0, sigma = 1, df = 50),
    size = 1
  ) +
  geom_function(
    color = "gray60",
    fun = function(x)
      dstudent_t(x, mu = 0, sigma = 4, df = 5),
    size = 1
  ) +
  geom_function(
    color = "gray40",
    fun = function(x)
      dstudent_t(x, mu = 2,  sigma = 1, df = 5),
    size = 1
  ) +
  xlab("x")

```

Next, we will implement the Bayesian analog of the linear model described earlier. Here's what the model formula will look like when implemented using the `bf()` function from the `brm` package:

```{r dataset1-bayesian-t-test-formula}
dataset1.brm.bayesiant.formula <- bf(effectiveness ~ condition - 1,
                                     family = student())
```

Let us take a minute to understand this code. It says that we are regressing the variable `condition` on `effectiveness`. The `- 1` removes the intercept term. The family argument is used to specify the probability distribution of the likelihood --- in other words, what is the distribution of $P(y)$.

### Step 1: Build the model (priors)

The blank ones are flat (uniform) priors. These are improper priors and usually needed to be set. We can and should adjust the priors given by `brm`.

```{r dataset1-bayesian-t-test-get-prior}
as_tibble(get_prior(dataset1.brm.bayesiant.formula, data = exp1.data))
```

Next, we need to specify priors for the parameters in the model. We will discuss briefly how these prior distributions were obtained.

For the prior on `b` which is the mean parameter of the Student's *t* distribution (used as the likelihood), an unbiased assumption would be that they should be centered around 5 (which is the center of the 9 possible answers), and the mean would likely be greater than 1 and less than 9 (unless every single participant responded either 1 or 9; but we know that was not the case).

The prior on `sigma` determines the dispersion parameter of the Student's *t* distribution. Now, considering that our data is bounded between 1 and 9, a uniform distribution will have the maximum variance given these constraints. The variance of such an uniform distribution would be: `sd(runif(1e4, 1, 9)` which is approximately 2.5. We know that our data will most likely have less variance than such a uniform distribution. Thus, we want the prior on `sigma` to assign very little probability mass for values greater than 2.5. Our prior `student_t(3, 0, 1)` assigns less than 0.05 probability to values greater than 2.5 (you can check by running the following code in the console: `sum(gamlss.dist::rTF(1e4, 0, 1, 3) > 2.5) / 1e4`).

```{r dataset1-bayesian-t-test-priors}
dataset1.brm.bayesiant.priors = c(
      prior(normal(5, 1.5), class = "b"),
      prior(student_t(3, 0, 1), class = "sigma")
  )
```

### Step 1: Build the model (prior predictive checks)

Before we implement the regression model, it is advisable to perform some prior predictive checks. Bayesian models are generative. In other words, it is possible to sample values from the prior distributions, feed them through the likelihood function to obtain a *prior predictive* distribution.

The prior predictive distribution should ideally resemble the data generating process, and should not assign probability mass to unlikely or impossible values. If the prior predictive distribution is assigning substantial probability mass to unlikely or impossible values, we should adjust our choice of prior distributions. Prior predictive checks also help make explicit some of the assumptions that go into the prior specification process.

Another important thing to note is that `brms` often specifies improper priors (denoted by `flat`) by default. If the priors are improper, we cannot sample draws from the prior predictive distribution.

The following code block implements some prior predictive checks:

```{r}
tibble(
  x = seq(0, 10, by = 0.01),
  y = dnorm(x, 5, 1.5)
) %>%
  ggplot(aes(x, y)) +
  geom_line(color = "#b8925f", size = 2) +
  scale_x_continuous(breaks = seq(1, 9, by = 1)) +
  labs(y = "Density") +
  theme(
    panel.grid.major.y = element_line(),
    axis.title.y = element_text(angle = 90, size = 24),
    axis.text = element_text(size = 20)
  )
```

```{r dataset1-bayesian-t-test-run-prior-predictive-check, fig.height=4, fig.width=9}
dataset1.brm.bayesiant.priorchecks <-
  brm(
    dataset1.brm.bayesiant.formula,
    prior = dataset1.brm.bayesiant.priors,
    data = exp1.data,
    backend = BRM_BACKEND,
    sample_prior = "only",
    file = "rds/dataset1.brm.bayesiant.priorchecks.rds"
  )

n_prior_draws <- 30

# extract n = 100 draws from the prior predictive distribution
dataset1.bayesiant.yprior <-
  posterior_predict(dataset1.brm.bayesiant.priorchecks, ndraws = n_prior_draws)

# the following computes the probability density for each "draw"
dim(dataset1.bayesiant.yprior) <- n_prior_draws * 123

# create a new table and assign .draw number
tibble(.value = dataset1.bayesiant.yprior,
       .draw = rep(1:n_prior_draws, times = 123)) %>%
  ggplot() +
  geom_density(
    aes(x = .value, group = .draw),
    color = DARK_PURPLE,
    alpha = .5,
    size = .5
  ) +
  labs(y = "", x = 'Draws from the prior predictive distribution')
```

Although, based on the above plot, it seems like the model is assigning some non-zero probability to impossible values (less than 1 or more than 9), these are primarily a result of our choice of the likelihood function --- the `student_t()` distribution does not allow us to set bounds on the predictive distribution. Another reason for extreme values may be the use of a Student's *t* prior for sigma with 3 degrees of freedom --- this distribution has fat tails and thus might result in predicting large values for the standard deviation parameter in our model.

We will revisit this point later. For now, let's move forward, and fit the model.

### Step 2: Computing the posterior probability distribution

Next, we compute the posterior probability distribution using `brms` and `Stan`. Depending on the complexity of your model, this step may take a lot of time. Our model is not so complex, hence this will not take too long :)

```{r dataset1-bayesian-t-test-run}
dataset1.brm.bayesiant =
  brm(
    dataset1.brm.bayesiant.formula,
    prior = dataset1.brm.bayesiant.priors,
    data = exp1.data,
    backend = BRM_BACKEND,
    # There are many other parameters you can play with.
    # Here we use default parameters to simplify the lecture.
    # We save the model in the following file.
    file = "rds/dataset1.brm.bayesiant.rds"
  )
```

### Step 3: Evaluate the fit

After the compilation step of the model is complete, we can evaluate the fit and examine the result. Similar to `lm()`, the first step is to call the `summary()` function on the variable which contains the model compilation result:

```{r dataset1-bayesian-t-summary}
summary(dataset1.brm.bayesiant)
```

This will give us an overview of the coefficients of the various parameters, along with the 95% credible intervals. This result shows that there is substantial overlap between the credible intervals for the two conditions. Before we delve deeper into the results, it is important to run some diagnostics on the sampling process.

### Step 3: Evaluate posterior predictions

First, we call `bayesplot` to draw posterior distributions and MCMC traces for each parameter. We want to check whether the four chairs have mixed well. If they have, this implies that the model has converged.

```{r dataset1-diagnotics-1}
color_scheme_set(scheme = "purple")
plot(dataset1.brm.bayesiant, newpage = T)
```

Next, we need to examine if the posterior distributions computed by our MCMC process resembles the data. Each draw from the posterior predictive distribution should roughly resemble a "hypothetical experiment" with the same experimental design. Thus, if our posterior predictive distribution looks very different from the data, it implies that something has gone awry in our model building step (Step 1).

They are many ways you can draw from the posterior predictive distribution. Here we use `posterior_predict` from `brms`.

```{r dataset1-posterior-diagnotics-2}
dataset1.bayesiant.y <- exp1.data$effectiveness

dataset1.bayesiant.yrep <-
  posterior_predict(dataset1.brm.bayesiant)

dataset1.bayesiant.yrepgroup <-
  posterior_predict(dataset1.brm.bayesiant, data.frame(condition = c('graph', 'no_graph')))

head(as_tibble(dataset1.bayesiant.yrep[,1:11]))
```

We use `bayesplot` to perform these diagnostic tests. First, we plot the first eight draws of posterior predictions and the original data. Again, there are many ways to do this (and these will be discussed in greater detail in Lecture 3).

```{r dataset1-posterior-diagnotics-3, fig.height=4, fig.width=10}
ppc_hist(y = dataset1.bayesiant.y,
         yrep = dataset1.bayesiant.yrep[1:8,],
         binwidth = .5)
```

An alternative approach would be to plot densities for each *draw* from the posterior predictive distribution superimposed with the density from the actual data:

```{r dataset1-posterior-diagnotics-4, fig.height=3, fig.width=10}
ppc_dens_overlay(y = dataset1.bayesiant.y,
                 yrep = dataset1.bayesiant.yrep[1:30,])
```

Finally, we look at whether our model is able to capture the summary statistics properly. Below we show the distribution of the estimated posterior mean for the two conditions, as well as the actual mean of the data in the two conditions. Since the actual mean appears to be centered around the estimated posterior distribution of the mean, we can conclude that it is roughly capturing the relevant information from the experimental data.

```{r dataset1-posterior-diagnotics-5, fig.height=3, fig.width=10}
ppc_stat_grouped(y = dataset1.bayesiant.y,
                 yrep = dataset1.bayesiant.yrep,
                 group = exp1.data$condition, binwidth = .1)
```

### Step 3: Comparing posterior distributions

Now that we have performed model diagnostics, we would like to interpret the results. The pertinent research question here is whether there is a difference in the effectiveness rating between the *graph* and *no graph* condition. We use conditional distributions to compare the mean difference between the two conditions. `add_epred_draws()` from the `tidybayes` package is the most convenient way.

```{r dataset1-t-test-posterior-fitted}
dataset1.bayesiant.posterior_epred <-
  tibble(condition = c('graph', 'no_graph')) %>%
  add_epred_draws(dataset1.brm.bayesiant,
                  re_formula = NA,
                  allow_new_levels = FALSE) %>%
  ungroup()

head(dataset1.bayesiant.posterior_epred)
```

Here `add_epred_draws()` takes two arguments:

-   *newdata*: is used to generate predictions. This variable should describe the experimental design. In this example, our experimental design consists of two conditions, *graph* and *no graph*, which is being passed as the first argument.

-   *object*: the model fit object

We then transform the data, subtract the two conditions from each other (using the `compare_levels()` function), and calculate the credible intervals (using `mean_qi()`).

```{r dataset1-bayesiant-posterior_comparison}
dataset1.bayesiant.posterior_comparison <-
  dataset1.bayesiant.posterior_epred %>%
  select(-c(.chain, .iteration, .row)) %>%
  compare_levels(variable = .epred, by = condition)

head(dataset1.bayesiant.posterior_comparison)

dataset1.bayesiant.posterior_comparison %>%
        mean_qi(.epred)
```

A basic way to interpret and communicate the results is to plot them. We plot this result using credible interval. In lecture 3, you will learn how to choose among these visualizations.

```{r dataset1-bayesiant-diff-1, fig.height=2, fig.width=5}
p3 = dataset1.bayesiant.posterior_epred %>%
  ggplot(aes(x = .epred, fill = condition)) +
  geom_density(
    color = NA,
    alpha = .5,
  ) +
  scale_fill_manual(values = COLOR_PALETTE) + 
  xlab('')

p3
```

```{r dataset1-bayesiant-diff-2, fig.height=2, fig.width=5}
dataset1.bayesiant.posterior_comparison %>%
  ggplot(aes(x = .epred)) +
  geom_density(
    fill = DARK_PURPLE,
    color = NA,
    alpha = .5,
  ) +
  geom_vline(aes(xintercept = 0), linetype = "dashed", color = "black") +
  coord_cartesian(xlim = c(-1, 1))  +
  ggtitle('no_graph - graph') + 
  xlab('difference in mean')
```

```{r dataset1-bayesiant-diffci, fig.height=2, fig.width=5}
dataset1.bayesiant.posterior_comparison %>%
  median_qi(.epred) %>%
  ggplot() +
  geom_point(aes(x = .epred, y = condition), color = DARK_PURPLE, size = 3) +
  geom_errorbarh(
    aes(xmin = .lower, xmax = .upper, y = condition),
    color = DARK_PURPLE,
    alpha = .5,
    size = 2,
    height = 0
  ) +
  geom_vline(aes(xintercept = 0), linetype = "dashed", color = "black") +
  coord_cartesian(ylim = c(0, 2), xlim = c(-1, 1))  +
  xlab('') + ylab('')
```

```{r dataset1-bayesiant-diff-prob}
dataset1.bayesiant.posterior_comparison %>%
  mutate(if_greater = .epred < 0) %>%
  summarise(`Pr(graph > no_graph)` = mean(if_greater))
```

Finally, we compare the results obtained from the Bayesian model to the frequentist estimates:

```{r dataset1-t-test-compare, fig.height=4, fig.width=10, warning=FALSE}
bind_rows(tidy.wrapper(dataset1.lm.freqt),
          tidy.wrapper(dataset1.brm.bayesiant)) %>%
  ggplot() +
  geom_pointrange(
    aes(
      color = model,
      y = estimate,
      ymin = conf.low,
      ymax = conf.high,
      x = term
    ),
    position = position_dodge(width = 0.2)
  ) +
  scale_color_brewer(palette = "Set1") +
  ylab('effectiveness') +
  scale_y_continuous(breaks = 1:9, limits = c(1, 9)) +
  coord_flip()
```

### Step 1: Build the model (likelihood and priors)

This model is the BEST test model as described by Kruschke in the paper *Bayesian estimation supersedes the t-test*. In this model, $\beta$ indicates the mean difference in the outcome variable between the two groups (in this case, the percent change in the BART scores). We fit different priors on $\beta$ and set different weights on these priors to obtain our posterior estimate.

$$
\begin{align}
y_{i} &\sim \mathrm{T}(\nu, \mu, \sigma) \\
\mu &= \alpha_{0} + \beta * x_i \\
\sigma &= \sigma_{a} + \sigma_{b}*x_i \\
\beta &\sim \mathrm{N}(\mu_{0}, \sigma_{0}) \\
\sigma_a, \sigma_b &\sim \mathrm{Cauchy}(0, 2) \\
\nu &\sim \mathrm{exp}(1/30)
\end{align}
$$

We translate the above specification to R code.

```{r dataset2-model1-formula}
dataset2.brm.student.formula <- bf(change ~ condition,
                                   sigma ~ condition,
                                   family = student())

head(as_tibble(get_prior(dataset2.brm.student.formula, data = dataset2)))
```

*IMPORTANT*

TL;DR: Avoid using Cauchy distributions as priors as it may result in pathological behavior. Use a Normal or Student's t distribution as prior instead. We go into further details below.

Note that, here, the authors Jansen and Hornbaek use a $\mathrm{Cauchy}(0, 2)$ prior on $\sigma$ based on the recommendation of Kruschke. While in theory, this distribution can be used as a proper prior distribution, in practice it is generally a bad idea to use Cauchy distributions as priors, especially for $\sigma$. The mean and variance of a Cauchy distribution are undefined, because it has "fat tails" (i.e. the values in its tails have high probability). As a result, when used as a prior, the sampler might end up with very small or very large values. Because the $\sigma$ parameter can only be positive, a log-link function is used i.e. the prior distribution is defined for $log(\sigma)$ as the log of a positive-only value spans the entire real number space. So, $\sigma = exp(x)$ where $x$ is a value sampled from the Cauchy prior. Because of such extreme values sampled from a Cauchy distribution, coupled with the exponential transformation, you might get values which are rounded to 0 or infinity.

```{r dataset2-bayesiant-prior-checks-1}
dataset2.brm.student.priorchecks = brm(
  dataset2.brm.student.formula,
  prior = c(
    prior(normal(0, 2), class = "b"),
    prior(cauchy(0, 2), class = "b", dpar = "sigma"),
    prior(exponential(0.033), class = "nu"),
    prior(student_t(3, 0, 5), class = "Intercept"),
    prior(student_t(3, 0, 1), class = "Intercept", dpar = "sigma")
  ),
  data = dataset2,
  backend = BRM_BACKEND,
  sample_prior = 'only',
  file = "rds/dataset2.brm.student.priorchecks.rds"
)
```

If we look at a histogram of the prior predictive distribution, we notice that it predicts really large values. Again, this is because the $\mathrm{Cauchy}(0, 2)$ distribution has a long tail. However, ignoring extreme values, if we calculate the 95% quantile interval of this prior prediction, we find that it is [-565.42, 614.32]. Compare this interval to the data range: [-36.64, 200.61], we see that although our predicted interval is wider, it is not too bad.

```{r dataset2-bayesiant-prior-checks-2}
min(dataset2$change)
max(dataset2$change)
dataset2.yprior <-  posterior_predict(dataset2.brm.student.priorchecks)

ggplot() +
  geom_histogram(aes(x = dataset2.yprior),
               fill = DARK_PURPLE,
               alpha = .5, 
               size = 1, 
               bins = 30,
               center = 0) +
 labs(x = 'Prior predictive distribution',  y = "") +
 theme(
   axis.text.y = element_blank()
 )

dim(dataset2.yprior) <- 4000 * 80

quantile(dataset2.yprior, probs = c(.025, .975))
```

### Step 2: Computing the posterior probability

```{r dataset2-bayesiant}
dataset2.brm.student = brm(
 bf(change ~ condition, 
 sigma ~ condition,
 family = student()),
 prior = c(
   prior(normal(0, 2), class = "b"),
   prior(cauchy(0, 2), class = "b", dpar = "sigma"),
   prior(exponential(.033), class = "nu"),
   prior(student_t(3, 0, 5), class = "Intercept"),
   prior(student_t(3, 0, 2), class = "Intercept", dpar = "sigma")
 ), 
 data = dataset2,
 backend = BRM_BACKEND,
 file = "rds/dataset2.brm.student.rds"
)

dataset2.brm.student$prior
```

### Step 3: Evaluate the fit (summary)

```{r dataset2-brm-student-summary}
summary(dataset2.brm.student)
```

### Step 3: Evaluate the posterior predictions

```{r dataset2-diagnotics-1, fig.height=8, fig.width=10}
plot(dataset2.brm.student, newpage = T)
```

```{r dataset2-posterior-diagnotics-2}

dataset2.student.y <- dataset2.brm.student$data$change
dataset2.student.yrep <- posterior_predict(dataset2.brm.student)

dataset2.student.epred <- tibble(condition = c(TRUE, FALSE)) %>%
  add_epred_draws(dataset2.brm.student,
                  re_formula = NA,
                  allow_new_levels = TRUE) %>%
  ungroup()
```

```{r dataset2-posterior-diagnotics-3, fig.height=3, fig.width=10}
ppc_hist(y = dataset2.student.y,
         yrep = dataset2.student.yrep[100:107, ],
         binwidth = 10)


ppc_dens_overlay(y = dataset2.student.y,
                 yrep = dataset2.student.yrep[2000:2030, ])


ppc_stat_grouped(
  y = dataset2.student.y,
  yrep = dataset2.student.yrep,
  group = dataset2$condition,
  binwidth = 5
)
```

### Step 3: Compare the posterior predictions

```{r dataset2-student-posterior-fitted}

dataset2.student.epred_comparison <-
  tibble(condition = c(TRUE, FALSE)) %>%
  add_epred_draws(dataset2.brm.student,
                  re_formula = NA,
                  allow_new_levels = FALSE) %>%
  ungroup() %>%
  select(-c(.chain, .iteration, .row)) %>%
  compare_levels(variable = .epred, by = condition) %>%
  rename(diff = .epred)


head(dataset2.student.epred_comparison)


```

```{r dataset2-studenyt-diffci, fig.height=1, fig.width=5}
dataset2.student.epred_comparison %>%
  select(diff) %>%
  mean_qi() %>%
  ggplot() +
  geom_point(aes(x = diff, y = condition), size = 3, color = DARK_PURPLE) +
  geom_errorbarh(
    aes(xmin = .lower, xmax = .upper, y = condition),
    height = 0,
    color = DARK_PURPLE,
    alpha = .5,
    size = 2
  ) +
  geom_vline(aes(xintercept = 0), linetype = "dashed", color = "gray") +
  coord_cartesian(ylim = c(0, 2), xlim = c(-5, 5))  +
  scale_y_discrete(label = c('expansive - not expansive')) +
  xlab('')  + ylab('')
```

## Compare variance

```{r dataset2-student-posterior-fitted-sigma}

dataset2.student.sigma.epred_comparison <-
  tibble(condition = c(TRUE, FALSE)) %>%
  add_epred_draws(dataset2.brm.student,
                  dpar = "sigma") %>%
  ungroup() %>%
  select(-c(.chain, .iteration, .row)) %>%
  compare_levels(variable = sigma, by = condition) %>%
  rename(sigma.diff = sigma)


head(dataset2.student.sigma.epred_comparison)


```

```{r dataset2-student-diff-sigma, fig.height=1, fig.width=5}
dataset2.student.sigma.epred_comparison %>%
  select(sigma.diff) %>%
  mean_qi() %>%
  ggplot() +
  geom_point(aes(x = sigma.diff, y = condition), size = 3, color = DARK_PURPLE) +
  geom_errorbarh(
    aes(xmin = .lower, xmax = .upper, y = condition),
    height = 0,
    color = DARK_PURPLE,
    alpha = .5,
    size = 2
  ) +
  geom_vline(aes(xintercept = 0), linetype = "dashed", color = "gray") +
  coord_cartesian(ylim = c(0, 2), xlim = c(-15, 15))  +
  scale_y_discrete(label = c('expansive - not expansive')) +
  xlab('')  + ylab('')
```

# Packages Information

```{r}

sessionInfo()

```
